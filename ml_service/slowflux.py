# -*- coding: utf-8 -*-
"""SlowFlux.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MqFCCEpsKxZ2-TGE8vseZSBj6asRHLCA
"""

import pandas as pd
import numpy as np

# 1. Load the datasets
df_benign = pd.read_csv('fastflux_benign_final.csv')
df_active = pd.read_csv('fastflux_active_final.csv')

# 2. Add the 'label' column
df_benign['label'] = 0
df_active['label'] = 1

# 3. Combine the DataFrames
df_combined = pd.concat([df_benign, df_active], ignore_index=True)

# 4. Calculate the key features
def calculate_num_a_records(a_records_str):
    if isinstance(a_records_str, str):
        return len(a_records_str.split(','))
    return 0

def calculate_min_ttl(ttl_values_str):
    if isinstance(ttl_values_str, str) and ttl_values_str:
        try:
            return min(map(int, ttl_values_str.split(',')))
        except ValueError:
            return np.nan  # Handle cases with non-numeric TTL values
    return np.nan

def calculate_num_ns_records(ns_records_str):
    if isinstance(ns_records_str, str):
        return len(ns_records_str.split(','))
    return 0

df_combined['Number of A Records'] = df_combined['a_records'].apply(calculate_num_a_records)
df_combined['Minimum TTL Value'] = df_combined['ttl_values'].apply(calculate_min_ttl)
df_combined['Number of NS Records'] = df_combined['ns_records'].apply(calculate_num_ns_records)
df_combined['Domain Length'] = df_combined['domain'].apply(len)

# Display the first few rows with the new features
print(df_combined.head())

# Identify the number of samples in each class
num_benign = len(df_combined[df_combined['label'] == 0])
num_active = len(df_combined[df_combined['label'] == 1])

print(f"Number of benign domains before balancing: {num_benign}")
print(f"Number of active domains before balancing: {num_active}")

# Determine the majority class and the number of samples to remove
if num_benign > num_active:
    majority_class = 0
    minority_class = 1
    n_oversample = num_benign - num_active
else:
    majority_class = 1
    minority_class = 0
    n_undersample = num_active - num_benign

# Undersample the majority class
df_majority = df_combined[df_combined['label'] == majority_class]
df_minority = df_combined[df_combined['label'] == minority_class]

df_majority_undersampled = df_majority.sample(n=len(df_minority), random_state=42) # Use a random_state for reproducibility

# Combine the undersampled majority class with the minority class
df_balanced = pd.concat([df_minority, df_majority_undersampled])

# Shuffle the balanced dataset
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# Verify the balanced class distribution
print("\nBalanced dataset distribution:")
print(df_balanced['label'].value_counts())

# Display the first few rows of the balanced DataFrame
print("\nFirst few rows of the balanced DataFrame:")
print(df_balanced.head())

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
features = ['Number of A Records', 'Minimum TTL Value', 'Number of NS Records', 'Domain Length']
X = df_balanced[features]
y = df_balanced['label']

# Split the data into training and testing sets (80/20 split with stratification)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Print the shapes of the resulting sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Verify the class distribution in the training and testing sets
print("\nClass distribution in training set:")
print(y_train.value_counts(normalize=True))

print("\nClass distribution in testing set:")
print(y_test.value_counts(normalize=True))

from sklearn.linear_model import LogisticRegression
import pickle

# 1. Instantiate the Logistic Regression model
model = LogisticRegression(random_state=42)

# 2. Train the model
model.fit(X_train, y_train)

print("Logistic Regression model trained successfully.")

# 3. Save the trained model to a file
filename = 'logistic_regression_model.pkl'
pickle.dump(model, open(filename, 'wb'))

print(f"Trained model saved to {filename}")

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# 1. Make predictions on the test data
y_pred = model.predict(X_test)

# 2. Calculate the evaluation metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

# 3. Print the evaluation metrics
print("Evaluation Metrics:")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"Accuracy: {accuracy:.4f}")

import re
import numpy as np
import pandas as pd  # Import pandas
from sklearn.linear_model import LogisticRegression
import pickle

# Load the trained model
try:
    with open('slowflux_model.pkl', 'rb') as file:
        model = pickle.load(file)
except FileNotFoundError:
    print("Error: logistic_regression_model.pkl not found. Make sure the model is saved.")
    exit()

def extract_features_from_dig_output(dig_output):
    """Extracts features from the raw dig output, including the domain."""
    num_a_records = 0
    min_ttl = np.inf
    num_ns_records = 0
    domain = None

    # Extract domain name (more robustly)
    for line in dig_output.splitlines():
        match_question = re.search(r';([\w.-]+)\.\s+IN\s+A', line)
        if match_question:
            domain = match_question.group(1)
            break
        match_answer = re.search(r'([\w.-]+)\.\s+\d+\s+IN\s+A\s+', line)
        if match_answer:
            domain = match_answer.group(1)
            break

    if not domain:
        return None, "Error: Could not extract domain name from dig output."

    # Extract A records and TTLs (more flexible whitespace)
    a_records = []
    ttls = []
    for line in dig_output.splitlines():
        match = re.search(rf'^{domain}\.\s*(\d+)\s*IN\s*A\s*([\d.]+)', line)
        if match:
            try:
                ttl = int(match.group(1))
                ttls.append(ttl)
                a_records.append(match.group(2))
            except ValueError:
                pass

    num_a_records = len(a_records)
    if ttls:
        min_ttl = min(ttls)
    else:
        min_ttl = np.nan

    # Extract NS records (more flexible whitespace and optional trailing dot)
    ns_records = []
    for line in dig_output.splitlines():
        match = re.search(rf'^{domain}\.\s*\d+\s*IN\s*NS\s*([\w.-]+)\.?', line)
        if match:
            ns_records.append(match.group(1).rstrip('.'))

    num_ns_records = len(ns_records)

    return {
        'domain': domain,
        'Number of A Records': num_a_records,
        'Minimum TTL Value': min_ttl,
        'Number of NS Records': num_ns_records,
        'Domain Length': len(domain) if domain else 0,
    }, None

def predict_from_dig_output(dig_output):
    """Predicts if a domain is 'Active' or 'Benign' based on dig output."""
    features_dict, error = extract_features_from_dig_output(dig_output)

    if error:
        return error

    if not features_dict or any(value is np.nan or value is np.inf for key, value in features_dict.items() if key != 'domain'):
        return "Error: Could not extract necessary information from dig output."

    # Ensure the order of features matches the training data
    feature_order = ['Number of A Records', 'Minimum TTL Value', 'Number of NS Records', 'Domain Length']
    input_data = pd.DataFrame([features_dict]).loc[:, feature_order] # Create DataFrame with feature order

    prediction = model.predict(input_data)[0]

    if prediction == 1:
        return "Active"
    else:
        return "Benign"

# Get user input from file (suggestion for handling input correctly)
try:
    with open("input.txt", "r") as f:
        dig_output = f.read()
except FileNotFoundError:
    print("Error: input.txt not found. Please create the file and add the dig output.")
    exit()

# Make the prediction
prediction = predict_from_dig_output(dig_output)

# Attempt to extract domain name for printing
extracted_domain = None
for line in dig_output.splitlines():
    match_question = re.search(r';([\w.-]+)\.\s+IN\s+A', line)
    if match_question:
        extracted_domain = match_question.group(1)
        break
    match_answer = re.search(r'([\w.-]+)\.\s+\d+\s+IN\s+A\s+', line)
    if match_answer:
        extracted_domain = match_answer.group(1)
        break

if extracted_domain:
    print(f"\nPrediction for {extracted_domain}: {prediction}")
else:
    print(f"\nPrediction: {prediction}")